# MockFactory.io - Production Readiness Report

**Report Date:** February 11, 2026
**Prepared by:** After Dark Systems AI Staff Enterprise Architect
**Status:** NOT READY FOR PRODUCTION - Staging deployment only
**Estimated Time to Production:** 2-3 weeks with focused effort

---

## Executive Summary

MockFactory.io has completed 8/23 critical security fixes and has solid foundational architecture. However, **significant bugs, missing features, and infrastructure gaps prevent production deployment**. This report identifies:

- **12 Critical Bugs** requiring immediate fixes
- **5 Revenue-Driving Features** to implement for Q1 growth
- **15 Remaining Security Issues** from the audit
- **Production Infrastructure Requirements**
- **Week-by-week roadmap** to launch

**Key Findings:**
- Security posture improved from 2/10 to 5/10 (target: 9/10 for production)
- Core provisioning logic is sound but has critical edge case bugs
- Missing database migration strategy will cause data loss
- No monitoring/observability = blind in production
- Stripe integration incomplete and vulnerable
- Auto-shutdown works but has timezone/race condition bugs

**Go/No-Go Recommendation:** NO-GO for production. Ready for staging deployment with alpha testers only.

---

## Part 1: Critical Bugs Identified

### BUG-001: Database Migration Strategy Missing (CRITICAL)
**Severity:** CRITICAL
**Impact:** Data loss on schema changes, failed deployments
**Location:** `app/main.py:21`

**Problem:**
```python
# app/main.py line 21
Base.metadata.create_all(bind=engine)
```

Using `create_all()` instead of Alembic migrations means:
- Schema changes will fail in production (tables already exist)
- No rollback mechanism for failed migrations
- No audit trail of database changes
- Cannot deploy updates without downtime

**Evidence:**
- `alembic/versions/` directory is empty (checked via ls -la)
- No `alembic.ini` configuration file
- Requirements.txt includes alembic==1.13.1 but it's not configured

**Root Cause:**
Development shortcut that must be fixed before production. Current approach assumes greenfield database every deployment.

**Fix Required:**
1. Initialize Alembic: `alembic init alembic`
2. Create initial migration from current models
3. Generate migration for each model change
4. Update deployment process to run `alembic upgrade head`
5. Remove `Base.metadata.create_all()` from main.py

**Test Cases:**
```bash
# Test migration creation
alembic revision --autogenerate -m "initial schema"

# Test upgrade
alembic upgrade head

# Test rollback
alembic downgrade -1

# Test from empty database
docker exec -it mockfactory-postgres psql -U mockfactory -c "DROP DATABASE mockfactory;"
docker exec -it mockfactory-postgres psql -U mockfactory -c "CREATE DATABASE mockfactory;"
alembic upgrade head
```

**Estimated Fix Time:** 4 hours

---

### BUG-002: Port Allocation Race Condition Still Exists (HIGH)
**Severity:** HIGH
**Impact:** Concurrent environment creation can still conflict
**Location:** `app/services/environment_provisioner.py:228-272`

**Problem:**
The port allocation logic has a TOCTOU (Time-of-Check-Time-of-Use) vulnerability:

```python
# Lines 241-249: Query all allocated ports
allocated_ports = self.db.query(PortAllocation.port).filter(
    PortAllocation.is_active == True
).all()
allocated_port_set = {p[0] for p in allocated_ports}

# Find first free port
for port in range(PORT_RANGE_START, PORT_RANGE_END + 1):
    if port not in allocated_port_set:  # CHECK
        # ...
        self.db.add(allocation)  # USE
```

**Race Condition Scenario:**
1. Request A queries allocated ports at T0, finds port 30000 free
2. Request B queries allocated ports at T1, finds port 30000 free
3. Request A commits port 30000 at T2 â†’ SUCCESS
4. Request B commits port 30000 at T3 â†’ UNIQUE CONSTRAINT VIOLATION â†’ RETRY

While the code handles the conflict with try/except, it's inefficient and could exhaust retries under high concurrency.

**Better Solution:**
Use database-level atomic operation:

```python
async def _get_available_port(self, environment_id: str, service_name: str) -> int:
    PORT_RANGE_START = 30000
    PORT_RANGE_END = 40000

    # Use SQL to atomically find and allocate port
    result = self.db.execute(text("""
        WITH available_port AS (
            SELECT generate_series(:start, :end) AS port
            EXCEPT
            SELECT port FROM port_allocations WHERE is_active = true
            LIMIT 1
        )
        INSERT INTO port_allocations (port, environment_id, service_name, is_active)
        SELECT port, :env_id, :service_name, true
        FROM available_port
        RETURNING port
    """), {
        "start": PORT_RANGE_START,
        "end": PORT_RANGE_END,
        "env_id": environment_id,
        "service_name": service_name
    })

    port = result.scalar_one_or_none()
    if not port:
        raise RuntimeError("No available ports")

    self.db.commit()
    return port
```

**Test Case:**
```python
# Simulate 50 concurrent environment creations
import asyncio

async def create_many():
    tasks = [create_environment_api() for _ in range(50)]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Verify no duplicate ports allocated
    allocated_ports = db.query(PortAllocation.port).all()
    assert len(allocated_ports) == len(set(allocated_ports))  # No duplicates
```

**Estimated Fix Time:** 2 hours

---

### BUG-003: Docker Socket Proxy Not Integrated (CRITICAL)
**Severity:** CRITICAL
**Impact:** Production deployment will fail, security vulnerability active
**Location:** `docker-compose.prod.yml` vs `app/services/environment_provisioner.py`

**Problem:**
The docker-compose.prod.yml defines a docker-proxy service and sets `DOCKER_HOST=tcp://docker-proxy:2375`, but the actual provisioner code doesn't respect this environment variable.

**Evidence:**
```python
# app/services/environment_provisioner.py line 213
result = subprocess.run(cmd, capture_output=True, text=True)
```

This uses the default Docker socket (`/var/run/docker.sock`) because:
1. `subprocess.run()` doesn't automatically use `DOCKER_HOST` env var
2. The environment variable is set in the API container, but subprocess doesn't inherit it
3. No explicit Docker client initialization

**Fix Required:**
```python
import os
import docker

class EnvironmentProvisioner:
    def __init__(self, db: Session):
        self.db = db
        # Use DOCKER_HOST if set, otherwise default socket
        docker_host = os.getenv('DOCKER_HOST', 'unix:///var/run/docker.sock')
        self.docker_client = docker.DockerClient(base_url=docker_host)

    async def _provision_container(self, ...):
        # Use docker-py library instead of subprocess
        container = self.docker_client.containers.run(
            image=service_config["image"],
            name=container_name,
            detach=True,
            ports={service_config['port']: host_port},
            environment=service_config["env"],
            command=service_config.get("command")
        )
        return {
            "container_id": container.id,
            "endpoint": endpoint,
            "host_port": host_port
        }
```

**Test Cases:**
```bash
# Test proxy integration
docker-compose -f docker-compose.prod.yml up -d docker-proxy

# Test API can connect through proxy
docker exec mockfactory-api python -c "
import docker
client = docker.DockerClient(base_url='tcp://docker-proxy:2375')
print(client.containers.list())
"

# Test privileged operations are blocked
docker exec mockfactory-api python -c "
import docker
client = docker.DockerClient(base_url='tcp://docker-proxy:2375')
try:
    client.images.pull('alpine')  # Should fail - IMAGES=0
    print('FAIL: Images allowed')
except:
    print('PASS: Images blocked')
"
```

**Estimated Fix Time:** 3 hours

---

### BUG-004: OCI Credentials Not Mounted (CRITICAL)
**Severity:** CRITICAL
**Impact:** S3/GCS/Azure emulation will fail in production
**Location:** `docker-compose.prod.yml` vs actual file system

**Problem:**
docker-compose.prod.yml expects OCI secrets at:
```yaml
secrets:
  oci_config:
    file: ${OCI_CONFIG_FILE:-./secrets/oci_config}
  oci_key:
    file: ${OCI_KEY_FILE:-./secrets/oci_key.pem}
```

But:
1. No `/secrets/` directory in the repository
2. No instructions on creating these files
3. Environment variables point to wrong paths:
   ```yaml
   - OCI_CONFIG_FILE=/app/config/oci_config  # Wrong mount point
   - OCI_KEY_FILE=/app/config/oci_key.pem    # Wrong mount point
   ```

Docker secrets mount to `/run/secrets/` by default, not `/app/config/`.

**Fix Required:**

1. Update docker-compose.prod.yml:
```yaml
environment:
  - OCI_CONFIG_FILE=/run/secrets/oci_config
  - OCI_KEY_FILE=/run/secrets/oci_key
```

2. Create secrets setup script:
```bash
#!/bin/bash
# scripts/setup-oci-secrets.sh

mkdir -p secrets

# Copy OCI config
cp ~/.oci/config secrets/oci_config

# Copy OCI key
cp ~/.oci/oci_api_key.pem secrets/oci_key.pem

# Set restrictive permissions
chmod 600 secrets/*

echo "OCI secrets configured. Never commit secrets/ directory!"
```

3. Add to .gitignore:
```
secrets/
```

**Test Cases:**
```bash
# Test OCI CLI works in container
docker exec mockfactory-api oci os bucket list --compartment-id <compartment>

# Should use /run/secrets/oci_config
docker exec mockfactory-api cat /run/secrets/oci_config
```

**Estimated Fix Time:** 1 hour

---

### BUG-005: Stripe Webhook Signature NOT Verified (CRITICAL)
**Severity:** CRITICAL - Listed in security audit but NOT FIXED
**Impact:** Replay attacks, fraudulent subscription manipulation
**Location:** `app/api/payments.py:172-189`

**Problem:**
The webhook handler attempts signature verification but has a critical flaw:

```python
# Line 183
event = stripe.Webhook.construct_event(
    payload, sig_header, settings.STRIPE_WEBHOOK_SECRET
)
```

This looks correct, but:
1. `settings.STRIPE_WEBHOOK_SECRET` might be empty/misconfigured
2. No logging of verification failures
3. No monitoring of webhook replay attempts
4. Exception handling might mask verification failures

**Better Implementation:**
```python
@router.post("/webhook")
async def stripe_webhook(
    request: Request,
    db: Session = Depends(get_db)
):
    """Handle Stripe webhook events with signature verification"""

    payload = await request.body()
    sig_header = request.headers.get('stripe-signature')

    # Validate signature header exists
    if not sig_header:
        logger.warning("Stripe webhook received without signature header")
        raise HTTPException(status_code=400, detail="Missing signature")

    # Validate webhook secret is configured
    if not settings.STRIPE_WEBHOOK_SECRET:
        logger.error("STRIPE_WEBHOOK_SECRET not configured!")
        raise HTTPException(status_code=500, detail="Webhook secret not configured")

    try:
        event = stripe.Webhook.construct_event(
            payload, sig_header, settings.STRIPE_WEBHOOK_SECRET
        )
        logger.info(f"Webhook verified: {event['type']}")
    except ValueError as e:
        logger.warning(f"Invalid webhook payload: {e}")
        raise HTTPException(status_code=400, detail="Invalid payload")
    except stripe.error.SignatureVerificationError as e:
        logger.warning(f"Webhook signature verification failed: {e}")
        # Alert security team
        await send_security_alert("stripe_webhook_signature_failed", {
            "ip": request.client.host,
            "signature": sig_header[:20] + "..."
        })
        raise HTTPException(status_code=400, detail="Invalid signature")

    # Log webhook for audit trail
    await log_webhook_event(event)

    # ... rest of handler
```

**Test Cases:**
```python
# Test with valid signature
response = client.post("/api/v1/payments/webhook",
    data=payload,
    headers={"stripe-signature": valid_signature}
)
assert response.status_code == 200

# Test with invalid signature
response = client.post("/api/v1/payments/webhook",
    data=payload,
    headers={"stripe-signature": "invalid"}
)
assert response.status_code == 400

# Test replay attack (same signature used twice)
# Should log security alert
```

**Estimated Fix Time:** 2 hours

---

### BUG-006: Auto-Shutdown Has Timezone Bug (MEDIUM)
**Severity:** MEDIUM
**Impact:** Environments shut down at wrong times, user frustration
**Location:** `app/services/background_tasks.py:60-62`

**Problem:**
```python
# Line 61
inactive_duration = datetime.utcnow() - env.last_activity
shutdown_threshold = timedelta(hours=env.auto_shutdown_hours)
```

Uses `datetime.utcnow()` but:
1. `env.last_activity` might be in a different timezone
2. No timezone awareness (uses naive datetime)
3. Daylight Saving Time not handled
4. No user timezone preference

**Scenario:**
- User in PST creates environment at 9am PST (5pm UTC)
- Sets auto_shutdown_hours = 4
- Last activity: 2026-02-11 17:00:00 (UTC, stored as naive datetime)
- Background task runs at 9pm UTC
- Calculates: 9pm - 5pm = 4 hours â†’ SHUTDOWN
- But only 2 hours have passed in user's timezone (11am PST)

**Fix Required:**
```python
from datetime import datetime, timezone

class Environment(Base):
    # Change columns to timezone-aware
    created_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
    started_at = Column(DateTime(timezone=True), nullable=True)
    last_activity = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
    user_timezone = Column(String, default="UTC")  # Store user timezone

# Update background task
async def auto_shutdown_task(self):
    now = datetime.now(timezone.utc)

    for env in running_envs:
        if env.last_activity.tzinfo is None:
            # Handle legacy naive datetimes
            last_activity = env.last_activity.replace(tzinfo=timezone.utc)
        else:
            last_activity = env.last_activity

        inactive_duration = now - last_activity
        # ... rest of logic
```

**Estimated Fix Time:** 2 hours

---

### BUG-007: Cloud Emulation Missing Database Dependency (HIGH)
**Severity:** HIGH
**Impact:** S3/GCS/Azure operations fail with database errors
**Location:** `app/api/cloud_emulation.py:177-178, 223-224, 260-261`

**Problem:**
Multiple endpoints update `environment.last_activity` and call `db.commit()` but don't have `db: Session = Depends(get_db)` in their function signature:

```python
# Line 136: s3_put_object
async def s3_put_object(
    bucket_name: str,
    object_key: str,
    file: bytes = File(...),
    content_type: Optional[str] = Header(None),
    environment: Environment = Depends(verify_environment_access)
):
    # ...
    environment.last_activity = datetime.utcnow()
    db.commit()  # ERROR: 'db' is not defined!
```

Same issue in:
- `s3_get_object()` (line 186)
- `s3_delete_object()` (line 229)
- `gcs_upload_object()` (line 318)
- `azure_put_blob()` (line 421)

**Fix Required:**
```python
async def s3_put_object(
    bucket_name: str,
    object_key: str,
    file: bytes = File(...),
    content_type: Optional[str] = Header(None),
    environment: Environment = Depends(verify_environment_access),
    db: Session = Depends(get_db)  # ADD THIS
):
    # ... rest of function
```

**Test Case:**
```python
# Test S3 PUT updates last_activity
env = create_test_environment()
initial_activity = env.last_activity

time.sleep(2)
client.put(f"/s3/{bucket}/key", data=b"test")

env_refreshed = db.query(Environment).get(env.id)
assert env_refreshed.last_activity > initial_activity
```

**Estimated Fix Time:** 30 minutes

---

### BUG-008: Password Exposure in Endpoints (MEDIUM)
**Severity:** MEDIUM - Security/Privacy concern
**Impact:** Database passwords exposed in API responses
**Location:** `app/services/environment_provisioner.py:142-179`

**Problem:**
Connection strings with passwords are stored in `environment.endpoints`:

```python
# Line 142
"connection_template": f"redis://:{redis_password}@localhost:{{port}}"

# Line 151
"connection_template": f"postgresql://postgres:{db_password}@localhost:{{port}}/testdb"
```

These are returned in API responses:
```json
{
  "id": "env-abc123",
  "endpoints": {
    "redis": "redis://:Xy$9f#mK2pQ@localhost:30145",
    "postgresql": "postgresql://postgres:Zq#8k$nL9rT@localhost:30146/testdb"
  }
}
```

**Issues:**
1. Passwords visible in API responses
2. Passwords logged if request/response logging enabled
3. Passwords sent over network (even with HTTPS, still in logs)
4. Client-side JavaScript could leak passwords

**Better Approach:**
Store passwords separately and return masked endpoints:

```python
# Store actual password in separate field
environment.credentials = {
    "redis": {"password": redis_password},
    "postgresql": {"password": db_password}
}

# Return masked endpoint
environment.endpoints = {
    "redis": "redis://localhost:30145",  # Password removed
    "postgresql": "postgresql://postgres@localhost:30146/testdb"
}

# Separate endpoint to retrieve credentials (requires auth)
@router.get("/{environment_id}/credentials")
async def get_credentials(environment_id: str):
    """
    Retrieve credentials for environment

    SECURITY: Only owner can retrieve credentials
    Returns one-time URL that expires after first use
    """
    # ... implementation
```

**Estimated Fix Time:** 3 hours

---

### BUG-009: No Container Health Checks (MEDIUM)
**Severity:** MEDIUM
**Impact:** Environments reported as "running" when services are down
**Location:** `app/services/environment_provisioner.py:213-226`

**Problem:**
Containers are started but never verified healthy:

```python
# Line 213
result = subprocess.run(cmd, capture_output=True, text=True)
if result.returncode != 0:
    raise RuntimeError(f"Failed to start {service_type} container: {result.stderr}")

container_id = result.stdout.strip()
```

This only checks if `docker run` command succeeded, not if the service inside actually started.

**Scenario:**
1. PostgreSQL container starts successfully
2. But PostgreSQL fails to initialize (corrupt data, port conflict, etc.)
3. Container is running but PostgreSQL is down
4. User gets "running" status but connections fail
5. User creates support ticket: "Environment broken!"

**Fix Required:**
```python
async def _provision_container(self, ...):
    # Start container
    container = self.docker_client.containers.run(...)

    # Wait for service to be healthy
    max_wait = 60  # seconds
    for i in range(max_wait):
        if await self._check_service_health(service_type, host_port):
            break
        await asyncio.sleep(1)
    else:
        # Timeout - service didn't become healthy
        container.stop()
        container.remove()
        raise RuntimeError(f"{service_type} failed health check after {max_wait}s")

    return container_info

async def _check_service_health(self, service_type: str, port: int) -> bool:
    """Check if service is responding"""
    if service_type == "redis":
        try:
            r = redis.Redis(host='localhost', port=port)
            r.ping()
            return True
        except:
            return False

    elif service_type in ["postgresql", "postgresql_pgvector", "postgresql_postgis"]:
        try:
            conn = psycopg2.connect(
                host='localhost',
                port=port,
                user='postgres',
                password=password,
                database='testdb',
                connect_timeout=3
            )
            conn.close()
            return True
        except:
            return False

    # ... other service types
```

**Estimated Fix Time:** 4 hours

---

### BUG-010: OCI Bucket Deletion Fails Silently (LOW)
**Severity:** LOW
**Impact:** Orphaned OCI resources, increased costs
**Location:** `app/services/environment_provisioner.py:396-413`

**Problem:**
```python
# Line 397
try:
    # Delete all objects first
    subprocess.run([
        "oci", "os", "object", "bulk-delete",
        "--bucket-name", bucket_name,
        "--force"
    ], capture_output=True)

    # Delete bucket
    subprocess.run([
        "oci", "os", "bucket", "delete",
        "--bucket-name", bucket_name,
        "--force"
    ], capture_output=True)
except Exception as e:
    print(f"Warning: Failed to delete {service_name} bucket: {e}")
```

Issues:
1. `subprocess.run()` doesn't raise exception on failure (need `check=True`)
2. Errors are only printed, not logged
3. No retry mechanism for transient failures
4. No verification that bucket was actually deleted
5. Failed deletions not tracked (can't retry later)

**Fix Required:**
```python
async def _delete_oci_bucket(self, bucket_name: str) -> bool:
    """Delete OCI bucket with retry logic"""

    try:
        # Delete all objects first
        result = subprocess.run([
            "oci", "os", "object", "bulk-delete",
            "--bucket-name", bucket_name,
            "--force"
        ], capture_output=True, text=True, check=True)

        logger.info(f"Deleted objects from bucket {bucket_name}")

        # Delete bucket
        result = subprocess.run([
            "oci", "os", "bucket", "delete",
            "--bucket-name", bucket_name,
            "--force"
        ], capture_output=True, text=True, check=True)

        logger.info(f"Deleted bucket {bucket_name}")
        return True

    except subprocess.CalledProcessError as e:
        logger.error(f"Failed to delete bucket {bucket_name}: {e.stderr}")

        # Queue for retry
        self.db.add(FailedDeletion(
            bucket_name=bucket_name,
            error=e.stderr,
            retry_count=0
        ))
        self.db.commit()
        return False

# Background task to retry failed deletions
async def cleanup_failed_deletions(self):
    """Retry failed OCI bucket deletions"""
    failed = self.db.query(FailedDeletion).filter(
        FailedDeletion.retry_count < 5
    ).all()

    for deletion in failed:
        if await self._delete_oci_bucket(deletion.bucket_name):
            self.db.delete(deletion)
        else:
            deletion.retry_count += 1
        self.db.commit()
```

**Estimated Fix Time:** 2 hours

---

### BUG-011: Background Tasks Not Gracefully Stopped (MEDIUM)
**Severity:** MEDIUM
**Impact:** Tasks continue running during shutdown, potential data corruption
**Location:** `app/services/background_tasks.py:43-92`

**Problem:**
```python
async def auto_shutdown_task(self):
    """Auto-shutdown environments that have been inactive"""
    while True:  # Infinite loop, no shutdown signal
        try:
            # ... task logic
        except Exception as e:
            logger.error(f"Error in auto-shutdown task: {e}")

        await asyncio.sleep(300)  # Sleep 5 minutes
```

Issues:
1. No graceful shutdown mechanism
2. If API server receives SIGTERM during database commit, could corrupt data
3. No way to wait for in-progress tasks to complete
4. Container restart loses track of what was being processed

**Fix Required:**
```python
class BackgroundTaskManager:
    def __init__(self):
        self.shutdown_event = asyncio.Event()
        self.tasks = []

    async def auto_shutdown_task(self):
        while not self.shutdown_event.is_set():
            try:
                # ... task logic
            except Exception as e:
                logger.error(f"Error: {e}")

            # Interruptible sleep
            try:
                await asyncio.wait_for(
                    self.shutdown_event.wait(),
                    timeout=300
                )
                break  # Shutdown signal received
            except asyncio.TimeoutError:
                continue  # Timeout, continue loop

        logger.info("Auto-shutdown task stopped gracefully")

    async def start_all_tasks(self):
        self.tasks = [
            asyncio.create_task(self.auto_shutdown_task()),
            asyncio.create_task(self.cleanup_destroyed_resources()),
            asyncio.create_task(self.billing_reconciliation()),
        ]
        await asyncio.gather(*self.tasks, return_exceptions=True)

    async def stop_all_tasks(self):
        """Gracefully stop all background tasks"""
        logger.info("Stopping background tasks...")
        self.shutdown_event.set()

        # Wait for tasks to complete (with timeout)
        await asyncio.wait_for(
            asyncio.gather(*self.tasks, return_exceptions=True),
            timeout=30
        )
        logger.info("All background tasks stopped")

# In main.py
@app.on_event("shutdown")
async def shutdown_event():
    """Gracefully stop background tasks on shutdown"""
    await background_manager.stop_all_tasks()
```

**Estimated Fix Time:** 2 hours

---

### BUG-012: Rate Limiting Redis Dependency Not Handled (MEDIUM)
**Severity:** MEDIUM
**Impact:** Rate limiting fails if Redis down, entire API stops working
**Location:** `app/middleware/rate_limit_middleware.py` (need to verify implementation)

**Problem:**
SlowAPI uses Redis for distributed rate limiting. If Redis goes down:
- Does rate limiting fail open (allow all requests)?
- Does it fail closed (deny all requests)?
- Does it fall back to in-memory?

Based on security fixes summary (line 239):
> "Fallback to in-memory if Redis unavailable"

This is good, but need to verify:
1. Fallback actually works
2. Fallback is tested
3. Logging when fallback active
4. Alerts when Redis connection lost

**Verification Needed:**
```python
# Test rate limiting with Redis down
docker stop mockfactory-redis

# Should still allow requests (with in-memory rate limiting)
response = client.get("/api/v1/environments")
assert response.status_code == 200

# Should log warning
assert "Redis connection failed, using in-memory rate limiting" in logs
```

**If fallback missing, fix required:**
```python
class GlobalRateLimitMiddleware(BaseHTTPMiddleware):
    def __init__(self, app):
        super().__init__(app)
        self.redis_client = None
        self.memory_limiter = {}  # Fallback in-memory storage
        self._setup_redis()

    def _setup_redis(self):
        try:
            self.redis_client = redis.Redis.from_url(settings.REDIS_URL)
            self.redis_client.ping()
        except Exception as e:
            logger.warning(f"Redis unavailable, using in-memory rate limiting: {e}")
            self.redis_client = None

    async def dispatch(self, request, call_next):
        # Try Redis first
        if self.redis_client:
            try:
                # Redis-based rate limiting
                # ...
            except redis.ConnectionError:
                logger.warning("Redis connection lost, falling back to in-memory")
                self.redis_client = None

        # Fallback to in-memory
        # ... in-memory rate limiting logic
```

**Estimated Fix Time:** 1 hour (if fallback already exists, just add monitoring)

---

## Part 2: Revenue-Driving Features for Q1 Growth

Based on the 6-tier pricing model ($0-$500/month) and target market (developers, teams, Fortune 500), here are the **top 5 features to maximize Q1 revenue**:

### FEATURE-001: Environment Snapshots & Cloning (HIGH PRIORITY)
**Business Justification:**
- Justifies Team tier ($150/month) upgrade from Developer ($50)
- Enables "Test Data as Code" workflow
- Reduces test setup time from hours to seconds
- Enterprise feature that Fortune 500 companies need

**Revenue Impact:**
- Move 30% of Developer users to Team tier = +$100/user/month
- 50 users Ã— $100 = **+$5,000 MRR**
- Enables Enterprise tier sales ($500+/month)

**Use Cases:**
```
Scenario 1: QA team needs consistent test data
- Create environment with perfect test dataset
- Snapshot it
- Clone for each test run
- Delete after test completes

Scenario 2: CI/CD pipeline
- Pre-baked snapshot with seeded data
- Each pipeline run clones snapshot
- Tests run against known-good state
- No flaky tests from data variability

Scenario 3: Team collaboration
- Senior dev creates complex data scenario
- Snapshots it with name "Bug-1234-repro"
- Junior dev clones snapshot to debug
- Saves hours of data setup time
```

**Implementation:**

```python
# API Endpoints
POST /api/v1/environments/{id}/snapshots
  - Pause environment
  - Export PostgreSQL: pg_dump
  - Export Redis: BGSAVE + copy RDB file
  - Upload to OCI Object Storage
  - Store metadata in database
  - Resume environment

GET /api/v1/environments/{id}/snapshots
  - List all snapshots for environment
  - Show size, date, description

POST /api/v1/snapshots/{snapshot_id}/clone
  - Create new environment from snapshot
  - Download dumps from OCI
  - Restore to new containers
  - Return new environment ID

DELETE /api/v1/snapshots/{id}
  - Delete snapshot from OCI
  - Remove metadata
```

**Database Schema:**
```python
class EnvironmentSnapshot(Base):
    __tablename__ = "environment_snapshots"

    id = Column(String, primary_key=True)
    environment_id = Column(String, ForeignKey("environments.id"))
    user_id = Column(Integer, ForeignKey("users.id"))
    name = Column(String, nullable=False)
    description = Column(String)

    # Snapshot data
    oci_bucket = Column(String)  # Where dumps are stored
    snapshot_data = Column(JSON)  # {redis_rdb: "path", pg_dump: "path"}
    size_bytes = Column(BigInteger)

    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    services = Column(JSON)  # Services that were snapshotted

    # Relationships
    environment = relationship("Environment")
    user = relationship("User")
```

**Pricing Integration:**
- FREE tier: 0 snapshots
- STARTER tier: 1 snapshot
- DEVELOPER tier: 3 snapshots
- TEAM tier: Unlimited snapshots
- BUSINESS tier: Unlimited + longer retention
- ENTERPRISE tier: Custom retention + compliance

**Complexity:** 3 days
**Dependencies:** None
**Risk:** Low

---

### FEATURE-002: CI/CD Integration SDK & GitHub Actions (HIGH PRIORITY)
**Business Justification:**
- PRIMARY conversion driver from free to paid
- Developer workflow integration = sticky users
- Enables "Test Before Merge" workflow
- Reduces trial-to-paid conversion time

**Revenue Impact:**
- 40% conversion improvement (10% â†’ 14%)
- 100 free users â†’ 14 paid (vs 10) = 4 extra
- 4 users Ã— $50/month = **+$200 MRR per 100 signups**
- At 500 signups/month: **+$1,000 MRR**

**Implementation:**

**1. GitHub Action:**
```yaml
# .github/workflows/test.yml
name: Test with MockFactory
on: [pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup MockFactory Environment
        uses: mockfactory/setup-environment@v1
        with:
          api_key: ${{ secrets.MOCKFACTORY_API_KEY }}
          services: |
            - postgresql_pgvector
            - redis
            - aws_s3
          snapshot: production-like-data  # Optional
        id: mockfactory

      - name: Run Tests
        env:
          DATABASE_URL: ${{ steps.mockfactory.outputs.postgresql_url }}
          REDIS_URL: ${{ steps.mockfactory.outputs.redis_url }}
          S3_ENDPOINT: ${{ steps.mockfactory.outputs.s3_endpoint }}
        run: |
          npm test

      - name: Cleanup MockFactory Environment
        if: always()
        uses: mockfactory/cleanup-environment@v1
        with:
          environment_id: ${{ steps.mockfactory.outputs.environment_id }}
```

**2. CLI Tool:**
```bash
# Install
npm install -g @mockfactory/cli

# Create environment from command line
mockfactory create --services postgresql,redis --snapshot my-snapshot
# Output: Environment env-xyz123 created
#         PostgreSQL: postgresql://postgres:***@localhost:30145/testdb
#         Redis: redis://localhost:30146

# Seed data
mockfactory seed env-xyz123 --template medical_patients --count 10000

# Destroy when done
mockfactory destroy env-xyz123
```

**3. Terraform Provider:**
```hcl
# terraform/main.tf
provider "mockfactory" {
  api_key = var.mockfactory_api_key
}

resource "mockfactory_environment" "test" {
  services = ["postgresql", "redis"]
  auto_shutdown_hours = 2

  snapshot = "staging-data"
}

output "database_url" {
  value = mockfactory_environment.test.endpoints.postgresql
  sensitive = true
}
```

**4. Python SDK:**
```python
# Python SDK for programmatic access
from mockfactory import MockFactory

client = MockFactory(api_key="mf_xxxxx")

# Create environment
env = client.environments.create(
    services=["postgresql", "redis"],
    snapshot="test-data"
)

# Get connection URLs
print(env.endpoints.postgresql)
print(env.endpoints.redis)

# Seed data
env.seed(template="medical_patients", count=5000)

# Cleanup
env.destroy()
```

**Complexity:** 5 days
**Dependencies:** API must be stable
**Risk:** Medium (documentation critical)

---

### FEATURE-003: Usage Dashboard & Cost Analytics (MEDIUM PRIORITY)
**Business Justification:**
- Reduces churn by preventing bill shock
- Enables upsell conversations ("You're hitting limits frequently...")
- Builds trust through transparency
- Helps users optimize spending

**Revenue Impact:**
- 10% churn reduction
- At 100 paying users: 10 fewer cancellations/month
- 10 users Ã— $50 avg = **+$500 MRR retained**
- Plus upsells from usage insights

**Features:**

**1. Real-Time Usage Dashboard:**
```
â”Œâ”€ Current Period (Feb 1-28) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Environment Hours:  47.3 / 100 (47% used)     â”‚
â”‚ API Requests:       3,241 / 5,000 (65%)       â”‚
â”‚ Data Generated:     12K / 50K records (24%)   â”‚
â”‚                                                â”‚
â”‚ Projected Month-End: 92 hours (within limit)  â”‚
â”‚ Days Remaining: 17                             â”‚
â”‚                                                â”‚
â”‚ [View Detailed Breakdown] [Download Report]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ Top Resource Consumers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. env-abc123 (prod-like-test)    24.1 hours  â”‚
â”‚ 2. env-def456 (integration-test)  15.8 hours  â”‚
â”‚ 3. env-ghi789 (dev-environment)    7.4 hours  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ Cost Breakdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Environment Hours:  $23.65                     â”‚
â”‚ Data Generation:    $3.20                      â”‚
â”‚ Storage (snapshots): $1.15                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚ Total This Month:   $28.00                     â”‚
â”‚ Tier Limit:         $50.00                     â”‚
â”‚ Remaining Budget:   $22.00                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**2. Cost Optimization Recommendations:**
```
ğŸ’¡ Cost Saving Opportunities:

1. env-abc123 has been running for 18 hours without activity
   Recommended: Enable auto-shutdown (save ~$6/month)

2. You're using PostgreSQL + Redis in 3 separate environments
   Recommended: Use snapshots to share data (save ~$12/month)

3. Your Team tier includes 300 hours, but you're using 92
   Recommendation: Downgrade to Developer tier? (save $100/month)
```

**3. Usage Alerts:**
```
Email: "You've used 80% of your API request limit"
Slack: "Environment env-abc123 has been running for 12 hours"
Webhook: POST to user's URL with usage data
```

**Implementation:**
```python
# New API endpoints
GET /api/v1/usage/current
  - Current period usage stats
  - Tier limits
  - Projected end-of-month

GET /api/v1/usage/history?period=last_6_months
  - Historical usage data
  - Trend analysis

GET /api/v1/usage/recommendations
  - Cost optimization suggestions
  - Based on usage patterns

POST /api/v1/usage/alerts
  - Configure usage alerts
  - Email, Slack, webhook
```

**Frontend Dashboard:**
```typescript
// React component for usage dashboard
import { LineChart, BarChart } from 'recharts'

export default function UsageDashboard() {
  const { usage } = useUsage()

  return (
    <div>
      <UsageGauge current={usage.hours} limit={usage.tier_limit} />
      <UsageTrend data={usage.history} />
      <CostBreakdown costs={usage.costs} />
      <Recommendations items={usage.recommendations} />
    </div>
  )
}
```

**Complexity:** 4 days (backend 2 days, frontend 2 days)
**Dependencies:** Frontend exists
**Risk:** Low

---

### FEATURE-004: Team Management & Collaboration (MEDIUM PRIORITY)
**Business Justification:**
- **CRITICAL** for Team tier ($150/month) justification
- Enables team sales (3-5 seats per company)
- Increases Average Revenue Per Account (ARPA)
- Sticky feature (teams don't switch platforms)

**Revenue Impact:**
- Team tier designed for 2-5 developers
- Average team size: 3 developers
- 3 developers Ã— $150 = **$450 vs $150 individual** = +$300/team
- 20 teams = **+$6,000 MRR**

**Features:**

**1. Team Workspaces:**
```python
class Team(Base):
    __tablename__ = "teams"

    id = Column(Integer, primary_key=True)
    name = Column(String, nullable=False)
    slug = Column(String, unique=True)  # mockfactory.io/team/acme-corp

    # Billing
    owner_id = Column(Integer, ForeignKey("users.id"))
    tier = Column(Enum(UserTier))
    stripe_subscription_id = Column(String)

    # Usage pooling
    total_hours_limit = Column(Integer)  # 300 hours shared
    hours_used_this_month = Column(Integer, default=0)

    # Relationships
    members = relationship("TeamMember", back_populates="team")
    environments = relationship("Environment", back_populates="team")

class TeamMember(Base):
    __tablename__ = "team_members"

    id = Column(Integer, primary_key=True)
    team_id = Column(Integer, ForeignKey("teams.id"))
    user_id = Column(Integer, ForeignKey("users.id"))
    role = Column(Enum(TeamRole))  # owner, admin, member

    # Permissions
    can_create_environments = Column(Boolean, default=True)
    can_manage_snapshots = Column(Boolean, default=True)
    can_view_billing = Column(Boolean, default=False)

    joined_at = Column(DateTime, default=datetime.utcnow)
```

**2. Team Sharing:**
```
â”Œâ”€ Team: Acme Corp â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Members: 4/5 seats                             â”‚
â”‚ Usage: 147 / 300 hours (49%)                   â”‚
â”‚                                                â”‚
â”‚ Environments:                                  â”‚
â”‚ â”œâ”€ staging-db (owned by alice@acme.com)        â”‚
â”‚ â”‚  â””â”€ Shared with: Bob, Charlie                â”‚
â”‚ â”œâ”€ integration-test (owned by bob@acme.com)    â”‚
â”‚ â”‚  â””â”€ Shared with: Team                        â”‚
â”‚ â””â”€ prod-replica (owned by charlie@acme.com)    â”‚
â”‚    â””â”€ Private (not shared)                     â”‚
â”‚                                                â”‚
â”‚ [Invite Member] [View Usage] [Manage Billing]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**3. API Changes:**
```python
# New endpoints
POST /api/v1/teams
  - Create team (requires Team tier subscription)

POST /api/v1/teams/{team_id}/members
  - Invite member by email

GET /api/v1/teams/{team_id}/environments
  - List team environments

POST /api/v1/environments
  - Add team_id parameter (optional)
  - If team_id: bill to team, share with members

GET /api/v1/teams/{team_id}/usage
  - Team usage dashboard
```

**Complexity:** 5 days
**Dependencies:** Billing system must support teams
**Risk:** Medium (multi-tenancy complexity)

---

### FEATURE-005: API Key Management & Programmatic Access (HIGH PRIORITY)
**Business Justification:**
- Already implemented (from security fixes)
- Just needs UI/UX and documentation
- Unlocks Developer â†’ Team tier upgrades
- Required for CI/CD integration

**Revenue Impact:**
- Enables FEATURE-002 (CI/CD integration)
- "Activation moment" for paid tiers
- Power users generate 3x more revenue
- **+$150 ARPU** for users who adopt API keys

**Missing Components:**

**1. API Key Management UI:**
```
â”Œâ”€ API Keys â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                â”‚
â”‚ Personal Access Tokens:                        â”‚
â”‚ â”œâ”€ GitHub Actions (created 5 days ago)         â”‚
â”‚ â”‚  â””â”€ mf_prod_xxxxxxxxxxxxxxxx                 â”‚
â”‚ â”‚  â””â”€ Last used: 2 hours ago                   â”‚
â”‚ â”‚  â””â”€ Scopes: environments:write               â”‚
â”‚ â”‚  â””â”€ [Revoke] [Edit Scopes]                   â”‚
â”‚ â”‚                                              â”‚
â”‚ â”œâ”€ Terraform Provider (created 12 days ago)    â”‚
â”‚ â”‚  â””â”€ mf_prod_yyyyyyyyyyyyyyyy                 â”‚
â”‚ â”‚  â””â”€ Last used: Never                         â”‚
â”‚ â”‚  â””â”€ Scopes: environments:*, snapshots:*      â”‚
â”‚ â”‚  â””â”€ [Revoke] [Edit Scopes]                   â”‚
â”‚ â”‚                                              â”‚
â”‚ â””â”€ [Create New API Key]                        â”‚
â”‚                                                â”‚
â”‚ Environment-Specific Keys:                     â”‚
â”‚ â”œâ”€ env-abc123 (staging-db)                     â”‚
â”‚ â”‚  â””â”€ mf_env_zzzzzzzzzzzzzzzz                 â”‚
â”‚ â”‚  â””â”€ Scopes: s3:*, data:read                  â”‚
â”‚ â”‚  â””â”€ [Revoke]                                 â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**2. Scoped Permissions:**
```python
class APIKey(Base):
    # Add scopes column
    scopes = Column(JSON, default=["*"])  # ["environments:read", "s3:write"]

    def has_permission(self, scope: str) -> bool:
        """Check if key has permission for scope"""
        if "*" in self.scopes:
            return True

        # Check exact match
        if scope in self.scopes:
            return True

        # Check wildcard (e.g., "environments:*" matches "environments:write")
        namespace = scope.split(":")[0]
        if f"{namespace}:*" in self.scopes:
            return True

        return False
```

**3. Usage Analytics:**
```
â”Œâ”€ API Key: GitHub Actions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Created: Jan 15, 2026                          â”‚
â”‚ Last Used: 2 hours ago from 34.203.123.45      â”‚
â”‚                                                â”‚
â”‚ Usage This Month:                              â”‚
â”‚ â”œâ”€ API Calls: 1,247 requests                   â”‚
â”‚ â”œâ”€ Environments Created: 89                    â”‚
â”‚ â”œâ”€ Data Seeded: 450,000 records                â”‚
â”‚                                                â”‚
â”‚ Recent Activity:                               â”‚
â”‚ â”œâ”€ 2 hours ago: Created env-abc123             â”‚
â”‚ â”œâ”€ 3 hours ago: Destroyed env-def456           â”‚
â”‚ â”œâ”€ 5 hours ago: Seeded 5,000 records           â”‚
â”‚                                                â”‚
â”‚ Security:                                      â”‚
â”‚ â”œâ”€ IP Whitelist: 34.203.0.0/16 (GitHub)        â”‚
â”‚ â”œâ”€ Rate Limit: 5,000 req/hour                  â”‚
â”‚ â”œâ”€ Expires: Never                              â”‚
â”‚                                                â”‚
â”‚ [Regenerate Key] [Revoke] [Edit Settings]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Complexity:** 3 days (mostly frontend)
**Dependencies:** API key backend (already done)
**Risk:** Low

---

## Summary: Revenue Feature Priorities

| Feature | Revenue Impact | Complexity | Priority | Quarter |
|---------|---------------|------------|----------|---------|
| Snapshots & Cloning | +$5,000 MRR | 3 days | HIGH | Q1 |
| CI/CD Integration | +$1,000 MRR | 5 days | HIGH | Q1 |
| API Key Management UI | +$150 ARPA | 3 days | HIGH | Q1 |
| Usage Dashboard | +$500 MRR | 4 days | MEDIUM | Q1 |
| Team Management | +$6,000 MRR | 5 days | MEDIUM | Q2 |

**Total Q1 Impact:** ~$6,650 MRR increase
**Total Development Time:** 20 days (4 weeks with 1 developer)

**Recommended Q1 Implementation Order:**
1. **Week 1:** API Key Management UI (unlocks CI/CD adoption)
2. **Week 2:** Snapshots & Cloning (flagship feature for Team tier)
3. **Week 3:** Usage Dashboard (reduces churn)
4. **Week 4:** CI/CD Integration SDK (conversion driver)

Team Management can wait until Q2 after proving Team tier demand.

---

## Part 3: Production Readiness Checklist

### Category 1: Security (15 remaining critical issues)

#### CRITICAL (Must fix before production)

1. **Stripe Webhook Signature Validation** âœ“ Fixed (but needs verification)
   - Status: Code exists but not tested
   - Action: Write integration tests
   - Time: 2 hours

2. **Database Migration Strategy** âŒ Not Fixed
   - Status: Using create_all() - will fail in production
   - Action: Implement Alembic migrations
   - Time: 4 hours
   - **BLOCKER**: Cannot deploy without this

3. **Docker Socket Proxy Integration** âŒ Not Fixed
   - Status: docker-proxy defined but not used
   - Action: Switch to docker-py library with DOCKER_HOST
   - Time: 3 hours

4. **OCI Credentials Mounting** âŒ Not Fixed
   - Status: Wrong mount paths
   - Action: Fix docker-compose secrets paths
   - Time: 1 hour

5. **Audit Logging** âŒ Not Fixed
   - Status: No audit trail for sensitive operations
   - Action: Implement audit log system
   - Time: 6 hours

#### HIGH (Should fix before production)

6. **Container Health Checks** âŒ Not Fixed
   - Status: No verification services are actually healthy
   - Action: Add health check polling after container start
   - Time: 4 hours

7. **Port Allocation Race Condition** âš ï¸ Partially Fixed
   - Status: Handles conflicts but inefficiently
   - Action: Use atomic SQL operation
   - Time: 2 hours

8. **Password Exposure in Endpoints** âŒ Not Fixed
   - Status: Passwords visible in API responses
   - Action: Separate credentials endpoint
   - Time: 3 hours

#### MEDIUM (Can defer to post-launch)

9. **Container Image Scanning** âŒ Not Fixed
   - Status: No CVE scanning
   - Action: Integrate Trivy in CI/CD
   - Time: 2 hours

10. **OCI Bucket Deletion Retry** âŒ Not Fixed
    - Status: Failed deletions not retried
    - Action: Add retry queue
    - Time: 2 hours

11. **Background Task Graceful Shutdown** âŒ Not Fixed
    - Status: No shutdown signal handling
    - Action: Add shutdown event
    - Time: 2 hours

12. **Auto-Shutdown Timezone Handling** âŒ Not Fixed
    - Status: Uses naive datetimes
    - Action: Switch to timezone-aware datetimes
    - Time: 2 hours

13. **Rate Limiting Redis Fallback** âœ“ Partially Fixed
    - Status: Claimed to exist, needs verification
    - Action: Test with Redis down
    - Time: 1 hour

14. **Cloud Emulation DB Dependency** âŒ Not Fixed
    - Status: Missing db parameter in endpoints
    - Action: Add db: Session = Depends(get_db)
    - Time: 30 minutes

15. **Security Headers** âŒ Not Fixed
    - Status: No CSP, HSTS, X-Frame-Options
    - Action: Add security headers middleware
    - Time: 2 hours

**Total Security Fixes Remaining: 40 hours (5 days)**

---

### Category 2: Infrastructure & Observability

#### CRITICAL

1. **Monitoring & Alerting** âŒ Not Implemented
   - Status: No visibility into production
   - Required:
     - Prometheus for metrics
     - Grafana for dashboards
     - AlertManager for alerts
   - Metrics needed:
     - Environment creation success/failure rate
     - Container provision time
     - API response times
     - Error rates by endpoint
     - Active environments count
     - OCI bucket operations
     - Database connection pool utilization
     - Auto-shutdown execution time
   - Time: 8 hours

2. **Centralized Logging** âŒ Not Implemented
   - Status: Logs scattered across containers
   - Required:
     - ELK stack OR CloudWatch Logs
     - Structured logging (JSON format)
     - Log aggregation from all containers
     - Log retention policy
   - Time: 6 hours

3. **Health Checks & Readiness Probes** âš ï¸ Partial
   - Status: Basic /health endpoint exists
   - Missing:
     - Database connectivity check
     - Redis connectivity check
     - OCI credentials verification
     - Docker daemon connectivity
   - Readiness vs Liveness distinction
   - Time: 3 hours

#### HIGH

4. **Backup & Disaster Recovery** âŒ Not Implemented
   - Status: No backup strategy
   - Required:
     - PostgreSQL automated backups (daily)
     - Point-in-time recovery capability
     - Redis RDB snapshots
     - OCI bucket versioning
     - Backup testing (monthly restore drill)
   - Time: 4 hours setup + ongoing

5. **Database Connection Pooling** âš ï¸ Unknown
   - Status: Need to verify if pgBouncer or SQLAlchemy pooling configured
   - Required for: 1000s of concurrent environments
   - Action: Verify pool settings in DATABASE_URL
   - Time: 2 hours

6. **Secret Management** âš ï¸ Partial
   - Status: Using .env files
   - Better: AWS Secrets Manager, HashiCorp Vault, or OCI Vault
   - Current approach: Environment variables
   - Concerns:
     - Secrets in version control? (need to check .gitignore)
     - Secrets in container environment (visible in docker inspect)
     - No secret rotation
   - Time: 6 hours to implement Vault

#### MEDIUM

7. **Rate Limiting Configuration** âœ“ Implemented but not tuned
   - Status: Tier-based limits exist
   - Missing:
     - Per-endpoint specific limits
     - DDoS protection
     - IP-based throttling for suspicious activity
   - Time: 2 hours tuning

8. **Database Performance Tuning** âŒ Not Done
   - Status: Using default PostgreSQL settings
   - Required:
     - Index analysis (EXPLAIN ANALYZE on common queries)
     - shared_buffers tuning
     - work_mem optimization
     - Connection pool sizing
   - Time: 4 hours

9. **CDN & Static Asset Delivery** âŒ Not Implemented
   - Status: nginx serves everything
   - Better: CloudFlare or CloudFront for static assets
   - Time: 3 hours

**Total Infrastructure Work: 38 hours (5 days)**

---

### Category 3: Operational Readiness

#### CRITICAL

1. **Deployment Runbook** âŒ Not Created
   - Required sections:
     - Pre-deployment checklist
     - Database migration steps
     - Service startup order
     - Health check verification
     - Rollback procedure
     - Post-deployment verification
   - Time: 4 hours to write

2. **Incident Response Plan** âŒ Not Created
   - Required:
     - On-call rotation
     - Escalation procedures
     - Critical incident playbooks
     - Customer communication templates
   - Time: 3 hours to write

3. **Load Testing** âŒ Not Done
   - Status: No performance baseline
   - Required tests:
     - 100 concurrent environment creations
     - 1000 API requests/second
     - Auto-shutdown with 500 environments
     - Database under load
   - Tools: k6, Locust, or Artillery
   - Time: 6 hours

#### HIGH

4. **Error Handling & User Feedback** âš ï¸ Partial
   - Status: Generic error messages
   - Better:
     - User-friendly error messages
     - Error codes for support
     - Actionable suggestions ("Try reducing count to 1000")
   - Time: 3 hours

5. **Documentation** âš ï¸ Partial
   - Status: Architecture and security docs exist
   - Missing:
     - API reference (OpenAPI/Swagger)
     - Integration guides
     - Troubleshooting guide
     - SDK documentation
   - Time: 8 hours

6. **Customer Support System** âŒ Not Implemented
   - Required:
     - Ticketing system (Zendesk, Intercom)
     - Knowledge base
     - Status page (status.mockfactory.io)
   - Time: 4 hours setup

**Total Operational Work: 28 hours (3.5 days)**

---

### Category 4: Billing & Compliance

#### CRITICAL

1. **Stripe Product/Price Setup** âš ï¸ Partial
   - Status: Code references STRIPE_PRICE_PROFESSIONAL but they're empty strings
   - Action: Run stripe_setup.py and configure .env
   - Time: 1 hour

2. **Usage Tracking Accuracy** âš ï¸ Needs Verification
   - Status: Usage logs created but billing reconciliation not implemented
   - Concerns:
     - Are usage logs accurate to the minute?
     - What happens if background task crashes during billing?
     - How to handle clock drift?
   - Action: Audit usage tracking logic
   - Time: 3 hours

3. **Failed Payment Handling** âš ï¸ Partial
   - Status: Webhook handles invoice.payment_failed
   - Missing:
     - Grace period before service cutoff
     - Retry logic
     - Customer notification
   - Time: 2 hours

#### HIGH

4. **Terms of Service / Privacy Policy** âŒ Not Created
   - Required before charging customers
   - Needs legal review
   - Time: 4 hours to draft (+ legal review time)

5. **GDPR Compliance** âŒ Not Addressed
   - Required for EU customers:
     - Data export capability
     - Right to deletion
     - Cookie consent
     - Privacy policy
   - Time: 8 hours

6. **Tax Handling** âŒ Not Implemented
   - Status: Stripe subscription doesn't include tax
   - Required: Stripe Tax or manual tax calculation
   - Time: 2 hours configuration

**Total Billing/Compliance: 20 hours (2.5 days)**

---

### Production Readiness Summary

| Category | Critical Items | High Items | Total Hours | Status |
|----------|---------------|-----------|-------------|---------|
| Security | 5 items | 3 items | 40 hours | âŒ NOT READY |
| Infrastructure | 3 items | 3 items | 38 hours | âŒ NOT READY |
| Operational | 3 items | 3 items | 28 hours | âŒ NOT READY |
| Billing/Compliance | 3 items | 3 items | 20 hours | âš ï¸ PARTIAL |
| **TOTAL** | **14 items** | **12 items** | **126 hours** | **âŒ NOT READY** |

**Minimum viable production:**
- Fix 14 critical items = 64 hours (8 days)
- Add basic monitoring = 8 hours (1 day)
- Write deployment runbook = 4 hours
- **Total: 76 hours (10 business days with focused effort)**

**Recommended production readiness:**
- Fix all critical + high items = 126 hours (16 days)
- Add comprehensive testing = 16 hours (2 days)
- **Total: 142 hours (18 business days = ~3.5 weeks)**

---

## Part 4: Implementation Roadmap

### Phase 1: Critical Fixes (Week 1-2)
**Goal: Make platform stable enough for limited beta**

#### Week 1: Security & Core Bugs
**Monday:**
- [ ] BUG-002: Fix port allocation race condition (2h)
- [ ] BUG-007: Add db dependency to cloud emulation (0.5h)
- [ ] BUG-001: Implement Alembic migrations (4h)
- [ ] Test migrations on staging (1h)

**Tuesday:**
- [ ] BUG-003: Integrate Docker socket proxy (3h)
- [ ] BUG-004: Fix OCI credentials mounting (1h)
- [ ] Test environment provisioning end-to-end (2h)
- [ ] BUG-009: Add container health checks (4h)

**Wednesday:**
- [ ] BUG-005: Enhance Stripe webhook validation (2h)
- [ ] BUG-011: Add graceful shutdown to background tasks (2h)
- [ ] BUG-006: Fix timezone handling in auto-shutdown (2h)
- [ ] Test auto-shutdown with multiple timezones (1h)

**Thursday:**
- [ ] Setup Prometheus + Grafana (4h)
- [ ] Configure critical alerts (2h)
- [ ] Implement structured logging (2h)

**Friday:**
- [ ] Setup CloudWatch Logs aggregation (2h)
- [ ] Write deployment runbook (3h)
- [ ] Security audit review (2h)
- [ ] Week 1 retrospective (1h)

**Week 1 Total: 40 hours**

#### Week 2: Infrastructure & Testing
**Monday:**
- [ ] Implement database backups (4h)
- [ ] Test backup restore procedure (2h)
- [ ] Configure database connection pooling (2h)

**Tuesday:**
- [ ] BUG-008: Separate credentials endpoint (3h)
- [ ] BUG-010: Add OCI deletion retry queue (2h)
- [ ] Implement security headers middleware (2h)

**Wednesday:**
- [ ] Load testing setup (k6 scripts) (3h)
- [ ] Run load tests: 100 concurrent environments (2h)
- [ ] Performance tuning based on results (3h)

**Thursday:**
- [ ] Stripe product/price configuration (1h)
- [ ] Usage tracking accuracy audit (3h)
- [ ] Failed payment handling improvements (2h)

**Friday:**
- [ ] Integration testing (all critical paths) (4h)
- [ ] Fix any bugs found (3h)
- [ ] Week 2 retrospective (1h)

**Week 2 Total: 40 hours**

**Phase 1 Deliverable: Platform ready for 10-20 alpha testers**

---

### Phase 2: Revenue Features (Week 3-4)
**Goal: Ship features that drive Q1 revenue**

#### Week 3: API Key UI + Snapshots
**Monday:**
- [ ] FEATURE-005: API key management UI (4h)
- [ ] FEATURE-005: API key scopes implementation (2h)
- [ ] FEATURE-005: Usage analytics dashboard (2h)

**Tuesday:**
- [ ] FEATURE-001: Snapshot creation endpoint (3h)
- [ ] FEATURE-001: PostgreSQL dump integration (2h)
- [ ] FEATURE-001: Redis RDB export (2h)

**Wednesday:**
- [ ] FEATURE-001: OCI upload for snapshots (2h)
- [ ] FEATURE-001: Snapshot restore/clone (3h)
- [ ] FEATURE-001: Snapshot management UI (3h)

**Thursday:**
- [ ] FEATURE-001: Snapshot testing (2h)
- [ ] FEATURE-001: Documentation (2h)
- [ ] FEATURE-001: Pricing tier restrictions (2h)

**Friday:**
- [ ] Integration testing (3h)
- [ ] Bug fixes (3h)
- [ ] Week 3 retrospective (1h)

**Week 3 Total: 40 hours**

#### Week 4: CI/CD Integration
**Monday:**
- [ ] FEATURE-002: GitHub Action development (4h)
- [ ] FEATURE-002: GitHub Action testing (2h)
- [ ] FEATURE-002: GitHub Action documentation (2h)

**Tuesday:**
- [ ] FEATURE-002: CLI tool (mockfactory-cli) (4h)
- [ ] FEATURE-002: CLI testing (2h)
- [ ] FEATURE-002: CLI documentation (2h)

**Wednesday:**
- [ ] FEATURE-002: Python SDK (4h)
- [ ] FEATURE-002: SDK testing (2h)
- [ ] FEATURE-002: SDK documentation (2h)

**Thursday:**
- [ ] FEATURE-003: Usage dashboard backend (3h)
- [ ] FEATURE-003: Usage dashboard frontend (3h)
- [ ] FEATURE-003: Cost optimization recommendations (2h)

**Friday:**
- [ ] End-to-end testing (all new features) (4h)
- [ ] Bug fixes (3h)
- [ ] Week 4 retrospective (1h)

**Week 4 Total: 40 hours**

**Phase 2 Deliverable: Revenue-driving features live**

---

### Phase 3: Production Launch Preparation (Week 5)
**Goal: Final polish and launch**

**Monday:**
- [ ] Terms of Service draft (2h)
- [ ] Privacy Policy draft (2h)
- [ ] Legal review coordination (1h)
- [ ] GDPR compliance audit (3h)

**Tuesday:**
- [ ] Customer support system setup (Intercom/Zendesk) (2h)
- [ ] Knowledge base creation (3h)
- [ ] Status page setup (status.mockfactory.io) (2h)

**Wednesday:**
- [ ] Final security penetration test (4h)
- [ ] Fix critical vulnerabilities (4h)

**Thursday:**
- [ ] Production deployment dry run (3h)
- [ ] Rollback procedure testing (2h)
- [ ] Incident response drill (2h)

**Friday:**
- [ ] Final documentation review (2h)
- [ ] Marketing materials preparation (2h)
- [ ] Launch checklist review (1h)
- [ ] **GO/NO-GO DECISION** (2h)

**Week 5 Total: 40 hours**

---

### Production Launch Timeline

```
Week 1-2 (Feb 11-22): Critical Fixes
â”œâ”€ Security vulnerabilities addressed
â”œâ”€ Monitoring & logging operational
â”œâ”€ Core bugs fixed
â””â”€ Alpha testing begins (10-20 users)

Week 3-4 (Feb 24-Mar 7): Revenue Features
â”œâ”€ API key management live
â”œâ”€ Snapshots & cloning ready
â”œâ”€ CI/CD integration shipped
â””â”€ Usage dashboard deployed

Week 5 (Mar 10-14): Production Prep
â”œâ”€ Legal docs finalized
â”œâ”€ Support system ready
â”œâ”€ Final security audit
â””â”€ Launch preparation

Week 6 (Mar 17): PRODUCTION LAUNCH
â”œâ”€ Limited beta (100 users)
â”œâ”€ Monitor stability
â”œâ”€ Collect feedback
â””â”€ Iterate rapidly

Week 7-8 (Mar 24-Apr 4): Scale & Optimize
â”œâ”€ Expand beta (500 users)
â”œâ”€ Performance optimization
â”œâ”€ Feature iteration
â””â”€ Full public launch (April 7)
```

---

## Risk Assessment

### CRITICAL RISKS

1. **Database Migration Failure** (PROBABILITY: HIGH)
   - **Impact:** Data loss, failed deployment, service outage
   - **Mitigation:**
     - Test migrations on staging with production data copy
     - Always backup before migration
     - Have rollback SQL scripts ready
     - Consider blue/green deployment

2. **Stripe Integration Issues** (PROBABILITY: MEDIUM)
   - **Impact:** Revenue loss, customer frustration
   - **Mitigation:**
     - Thorough webhook testing
     - Manual invoice reconciliation initially
     - Monitor Stripe dashboard daily
     - Test failed payment scenarios

3. **OCI Resource Exhaustion** (PROBABILITY: MEDIUM)
   - **Impact:** Unable to create new environments
   - **Mitigation:**
     - Monitor OCI compartment limits
     - Set up alerts at 80% capacity
     - Have expansion plan ready
     - Implement queue for resource-intensive operations

### HIGH RISKS

4. **Auto-Shutdown Bugs** (PROBABILITY: MEDIUM)
   - **Impact:** Environments shut down prematurely, angry customers
   - **Mitigation:**
     - Conservative default (4 hours)
     - Email notification before shutdown
     - Manual override capability
     - Extensive timezone testing

5. **Docker Socket Exposure** (PROBABILITY: LOW)
   - **Impact:** Complete host compromise
   - **Mitigation:**
     - Docker socket proxy (already designed)
     - Regular security audits
     - Container runtime monitoring
     - Intrusion detection system

6. **Rate Limiting Bypass** (PROBABILITY: LOW)
   - **Impact:** DoS attacks, resource exhaustion
   - **Mitigation:**
     - Multiple layers of rate limiting
     - IP-based throttling
     - DDoS protection (CloudFlare)
     - Circuit breakers

### MEDIUM RISKS

7. **Performance Degradation Under Load** (PROBABILITY: MEDIUM)
   - **Impact:** Slow response times, failed provisions
   - **Mitigation:**
     - Load testing before launch
     - Auto-scaling for API servers
     - Database read replicas
     - Redis caching

8. **Customer Support Overwhelm** (PROBABILITY: HIGH)
   - **Impact:** Poor user experience, churn
   - **Mitigation:**
     - Comprehensive documentation
     - Interactive tutorials
     - Automated onboarding
     - Community forum

---

## Go/No-Go Criteria

### GO Criteria (all must be met)

#### Security
- [ ] All CRITICAL security issues resolved
- [ ] Stripe webhook signature validation tested
- [ ] Docker socket proxy integrated and tested
- [ ] No hardcoded secrets in code
- [ ] Audit logging operational

#### Stability
- [ ] Database migrations tested on staging
- [ ] 100 concurrent environment test passed
- [ ] Auto-shutdown works correctly for 24 hours
- [ ] No data loss in stress testing
- [ ] All critical bugs fixed

#### Monitoring
- [ ] Prometheus + Grafana dashboards live
- [ ] Critical alerts configured
- [ ] Logs aggregated and searchable
- [ ] Health checks operational
- [ ] On-call rotation established

#### Business
- [ ] Stripe products/prices configured
- [ ] Terms of Service finalized
- [ ] Privacy Policy published
- [ ] Support system ready
- [ ] Backup/restore tested

### NO-GO Triggers (any of these = delay launch)

- [ ] Failed penetration test
- [ ] Data loss in testing
- [ ] Stripe integration not working
- [ ] No monitoring/alerting
- [ ] Database migration strategy not implemented
- [ ] Critical security vulnerability unresolved
- [ ] Legal docs not approved
- [ ] Unable to handle 100 concurrent users

---

## Recommended Action Plan

### Immediate Next Steps (This Week)

**Tuesday (Today):**
1. Set up staging environment
2. Fix BUG-001: Implement Alembic migrations (CRITICAL)
3. Fix BUG-007: Add db dependency to cloud emulation endpoints

**Wednesday:**
4. Fix BUG-003: Integrate Docker socket proxy
5. Fix BUG-004: Fix OCI credentials mounting
6. Test environment provisioning end-to-end

**Thursday:**
7. Fix BUG-009: Implement container health checks
8. Fix BUG-002: Optimize port allocation
9. Begin monitoring setup (Prometheus)

**Friday:**
10. Complete monitoring setup (Grafana dashboards)
11. Fix BUG-005: Enhanced Stripe webhook validation
12. Run smoke tests on staging

### Decision Points

**End of Week 2 (Feb 22):**
- **GO/NO-GO for Alpha Testing**
- Criteria: All critical bugs fixed, basic monitoring operational
- If GO: Invite 10-20 alpha testers
- If NO-GO: Extend critical fixes for 1 more week

**End of Week 4 (Mar 7):**
- **GO/NO-GO for Limited Beta**
- Criteria: Revenue features shipped, no critical incidents in alpha
- If GO: Open to 100 beta users
- If NO-GO: Focus on stability over features

**End of Week 5 (Mar 14):**
- **GO/NO-GO for Production Launch**
- Criteria: All GO criteria met, legal docs finalized
- If GO: Launch to public
- If NO-GO: Extend beta, fix blockers

### Resource Requirements

**Developers:**
- 1 senior full-stack engineer (primary)
- 1 DevOps engineer (part-time for infrastructure)
- 1 frontend engineer (part-time for dashboards/UI)

**Other Resources:**
- Legal review (Terms of Service, Privacy Policy)
- Security consultant (penetration testing)
- Technical writer (documentation)

**Estimated Total Cost:**
- Engineering: $30,000 (3 weeks @ $10k/week blended rate)
- Legal: $2,000
- Security audit: $3,000
- Infrastructure: $500/month (OCI + monitoring)
- **Total: ~$35,500 to production launch**

---

## Conclusion

MockFactory.io has **solid architecture and significant potential**, but is **not ready for production deployment**. The platform needs:

1. **2-3 weeks of focused engineering** to fix critical bugs and security issues
2. **Comprehensive monitoring and observability** to operate confidently
3. **Revenue-driving features** to justify paid tier upgrades
4. **Load testing and performance optimization** for scale

**Recommended Path:**
- âœ… **Deploy to staging immediately** (today/tomorrow)
- âœ… **Alpha test with 10-20 users** (starting Week 2)
- âœ… **Limited beta with 100 users** (Week 4-5)
- âœ… **Production launch** (Week 6 - March 17, 2026)
- âœ… **Full public launch** (Week 8 - April 7, 2026)

This aggressive timeline is achievable IF:
- Development team is dedicated full-time
- No major blockers discovered in testing
- Alpha testers provide quick feedback
- Legal review happens in parallel

**If any critical issues emerge, extend timeline by 1-2 weeks.**

**Current Status: 5/10 production readiness**
**Target for Launch: 9/10 production readiness**
**Estimated Time: 18 business days (3.5 weeks)**

---

**Report prepared by:** After Dark Systems AI Staff Architect
**Date:** February 11, 2026
**Next Review:** February 18, 2026 (end of Week 1)
**Contact:** Ryan (ryan@afterdarksystems.com)
